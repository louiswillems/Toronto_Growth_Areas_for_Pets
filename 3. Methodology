Overview of clean up, EDA and machine learning processes used:

Data Clean up:
--------------
* check Datatypes and convert any fields that are string to float (i.e. the Dog and Total counts from the Toronto pet licensing data set were imported as object/string but need to be converted to float)
* check for and drop null/na rows
* dropped all the extraneous columns from the Population / Census data set except for the FSA and Population in 2016

Retrieval of Neighbourhood names
---------------------------------
* the Toronto Pet Licensing data came organized by FSA but assigned no names to those areas.  To get that information, 
I used Beautiful Soup to scrape the Wikipedia page (https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M)
* the Wikipedia page has many rows for each FSA (Since many of them span multiple neighbourhoods) and I wanted to have one row per FSA
with all the included neighborhood names.  So I looped through the rows in the html table, checked if the FSA was already captured"
   * yes: append to list of neighhourhoods for that FSA
   * no: start a new row
* when merging the various datasets together, I merged on FSA and again dropped any null rows

Retrieval of Latitude/Longitude for each FSA
--------------------------------------------
* used GeoPy to retrieve lat/long for each FSA

Check for Outliers
------------------
* I found one outlier in the dataset, which was FSA=M5W that had a population of only 15 people.  This is significantly lower than 
   the population of any other FSA (the next lowest had a population of 2000) and would skew the results.  So I dropped this row
   
   
Exploratory Data Analysis
--------------------------
* In the EDA, I calculated and added a number of new columns:
   PropTotal_2017: proportion of the licenese granted in Toronto in 2017 that were granted to this FSA
   PropDogs: of the licenses granted in this FSA, what percentage were granted for Dogs vs Cats?
   PetChange: Total Pets 2017 - Total Pets 2013
   PropTotal_2013: proportion of the liceneses granted in Toronto in 2017 that were granted to this FSA
   PetChange_Prop: PropTotal_2017 - PropTotal_2013 (change in proportion of pets belong to this FSA)
   PerCapitaPetsTotal: pets/pop in FSA (number of new pets registered in FSA per person
   Venues: retrieved from FourSquare (total pet stores and services in FSA)
   PetServicesPerPerCapitaPet: Venues/PerCapitaPetsTotal rough estimate of how many services exist per PerCapitapet
* I produced a number of Choropleth diagrams showing the above data (comparing licenses issued across the FSAs in 2017, changes
in license counts between 2013 and 2017 and per capita pets)

Retrieval of FourSquare data
----------------------------
* After retrieving the FourSquare data on pet servivces/stores, I did a check to see how many pet stores were being counted as
belonging to more than one FSA.  I found that there were some stores  being counted twice, but decided that it doesn't matter for the
purposes of this analysis.  If a store is within a 1000m of more than 1 FSA, then it's still local and usable by people in those
FSAs and that's what we care about here.  


Machine Learning
---------------
For this analysis, I used the KMeans algorithm to attempt to cluster FSA areas based on three factores:
* Population
* Total Pets registered in 2017
* number of Existing Pet store/services

I wanted to try to isolate FSAs which share characteristics suggesting that they're good candidates for a new pet store/service: 
fairly high population, high number of pets and low number of existing services

Since population, total pets and number of venues are all on very different scales, I used StandardScaler to scale the results based on 
standard deviations from the mean

To find the appropriate number of clusters, I used the visual Elbow method to find the point at which the score levels off -- meaning 
that adding more clusters doesn't dramatically reduce the error any further.  I found that 12 clusters looked to be correct for this data

